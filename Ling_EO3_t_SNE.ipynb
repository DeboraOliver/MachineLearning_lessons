{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxZTL42AOpgUCsOWIi4KUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeboraOliver/MachineLearning_lessons/blob/main/Ling_EO3_t_SNE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudo Orientado 3 - t-SNE\n",
        "\n",
        "Você deve criar um programa em Python que vai ler o banco de dados Iris, armazenar os dados em um Pandas DataFrame, separar as características da variável target, treinar o modelo do **t-SNE para três (3) componentes e 1500 iterações com o Random_state=0.**\n",
        "\n",
        "Após isso, você deve alterar a quantidade de **componentes do t-SNE para um (1) e manter as iterações em 1500, na sequência, aumenta as iterações para 5000.** Você deve imprimir o valor da métrica divergência de Kullback-Leibler para cada combinação de parâmetros que usar. \n",
        "\n",
        "Feito isso, colete os valores da métrica divergência de Kullback-Leibler e compare para saber qual combinação de parâmetros apresentou a menor taxa de erro, ou seja, qual abordagem obteve o menor valor da métrica divergência de Kullback-Leibler. \n",
        "\n",
        "Para finalizar, em um breve texto, responda as questões a seguir:\n",
        "\n",
        "Qual abordagem é a melhor? <br>\n",
        "Usar menos componentes é melhor ou pior?<br>\n",
        "A quantidade de iterações influencia na métrica de erro?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aHmYVCrODqUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#ignorando os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "sZcS9fazEi7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHoANU5sDf9m"
      },
      "outputs": [],
      "source": [
        "def t_sne(numero_componentes, numero_iteracoes, randomizacao):\n",
        "\n",
        "  #parte 1: preparação dos dados\n",
        "\n",
        "  # fAZ O DOWNLOAD DOS DADOS E SALVA NO DATAFRAME\n",
        "  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "  df = pd.read_csv(url, names=['sepal length','sepal width','petal length', 'petal width','target'])\n",
        "  #define as caracteristicas\n",
        "  features = ['sepal length','sepal width','petal length', 'petal width']\n",
        "  caracteristicas = df.loc[:, features].values\n",
        "  target = df.loc[:,['target']].values\n",
        "\n",
        "  #inicializando o t-sne, definindo a quantidade de componentes que serão criados e o numero de iterações.\n",
        "  tsne = TSNE(n_components= numero_componentes, n_iter = numero_iteracoes, random_state= randomizacao)\n",
        "  caracteristicas_tsne = tsne.fit_transform(caracteristicas)\n",
        "  # caracteristicas_tsne\n",
        "\n",
        "  #salva os dados sno pandas dataframe\n",
        "  principal_df_tsne = pd.DataFrame(data = caracteristicas_tsne)\n",
        "  #Concatena o dataframe gerado com os componentes princixpais e anexa a vaariável target\n",
        "  final_DF_tsne = pd.concat([principal_df_tsne, df[['target']]], axis = 1)\n",
        "\n",
        "  #Imprime  a \"qualidade\" do produto gerado. A métrica divergência de KL representa o erro, quanto menor o valor dela, melhor  o modelo.\n",
        "  #Quanto menor for o valor dessa métrica, melhor é o modelo\n",
        "  \n",
        "  return print('Kullback-Leibler divergence t-SNE: ', tsne.kl_divergence_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Situação 1 : t-SNE para três (3) componentes e 1500 iterações com o Random_state=**0**"
      ],
      "metadata": {
        "id": "o7YFoHhcFRqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_sne(numero_componentes = 3 , numero_iteracoes = 1500, randomizacao =0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs0lTbSrEpU7",
        "outputId": "8bd8f509-c595-42cc-d8ea-a31f8c02abac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullback-Leibler divergence t-SNE:  0.7685528993606567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Situação 2 : componentes do t-SNE para um (1) e manter as iterações em 1500"
      ],
      "metadata": {
        "id": "ghyXtLbtGkRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_sne(numero_componentes = 1 , numero_iteracoes = 1500, randomizacao =0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrXCqu23GRPu",
        "outputId": "ea39c439-5f90-4000-b0e9-9edac99c2f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullback-Leibler divergence t-SNE:  0.6063571572303772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Situação 3 : componentes do t-SNE para um (1) e manter as iterações em 5000"
      ],
      "metadata": {
        "id": "YiRhKeBzG-YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_sne(numero_componentes = 1 , numero_iteracoes = 5000, randomizacao =0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-fszMi2G9qC",
        "outputId": "aea0b8e9-0510-4331-b47e-bfc9e8f0749d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullback-Leibler divergence t-SNE:  0.6063294410705566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão\n",
        "A métrica de Kullback-Leibler é usada para avaliar a quantidade de erros que pode haver na transformação dos dados quando aplicado ao problema de redução de dimensões. O objetivo desta forma é  um valor de KL mais baixo possível. Isto é, quanto menor for o valor dessa métrica, melhor é o modelo. Portanto, no exercício acima a melhor abordagem foi obtida quando reduzimos o número de componente de 3 para 1 e aumentamos a quantidade de iterações de 1500 para 5000. Entretando, nota-se que quando aumentamos  a quantidade de iterações de 1500 para 5000, o KL varia muito pouco, pois a técnica já está chegando próximo ao melhor valor possível."
      ],
      "metadata": {
        "id": "RXJebJNQIINZ"
      }
    }
  ]
}